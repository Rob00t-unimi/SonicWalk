<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SonicWalk</title>
    <style>
        body {
            margin: 0;
            padding: 0;
            font-family: Arial, sans-serif;
        }

        .navbar {
            background-color: #333;
            overflow: hidden;
            position: fixed;
            width: 100%;
            top: 0;
        }

        .navbar a {
            float: left;
            display: block;
            color: white;
            text-align: center;
            padding: 14px 20px;
            text-decoration: none;
        }

        .navbar a:hover {
            background-color: #ddd;
            color: black;
        }

        .navbar a.active {
            background-color: #4CAF50;
            color: white;
        }

        #pdfContainer {
            height: 83vh;
        }

        #home, #guide, #analysis, #credits {
            padding-top: 8vh;
            padding-left: 2vh;
        }

        #home {
            text-align: center;
        }

        #readme_container {
            padding-left: 50vh;
            padding-right: 50vh;
        }

        pre code {
        padding: 0;
        border: none;
        border-radius: 0;
        color: rgb(198, 198, 198)
        }
        pre {
            padding: 10px;
            background-color: rgba(0, 0, 0, 0.893);
            border-radius: 5px;
        }
        .lang-python {
            color: rgb(224, 224, 224)
        }
        .lang-python .operator {
        color: rgb(121, 188, 44)
        }
        .lang-python .string {
        color: #09c8c8; /* Colore delle stringhe */
        }
        .lang-python .object {
        color: #ff5f00; /* Colore dei numeri */
        }
        .lang-python .number {
        color: #cc00cc; /* Colore degli operatori */
        }
        .lang-python .comment {
        color: #888; /* Colore dei commenti */
        }
        .lang-python .value {
        color: #ef910e; /* Colore dei commenti */
        }

    </style>

</head>
<body>

<div class="navbar">
  <a class="active" href="#home">Home</a>
  <a href="#guide">Guide to using the app</a>
  <a href="#analysis">Analysis Documentation</a>
  <!-- <a href="#credits">Credits</a> -->
</div>

<h1 id="home">SonicWalk</h1>
<div id="readme_container">

    <p>SonicWalk is an innovative healthcare application designed to simplify and engage patients during rehabilitative exercises using real-time sonification. Developed in <strong>Python</strong> and <strong>PyQT5</strong> to ensure cross-platform compatibility, the application allows medical staff to manage patient records, monitor exercise sessions, and visualize results. It utilizes <strong>Movella MTw Awinda</strong> motion trackers and their respective APIs to gather necessary data.</p>
    <h3 id="features">Features</h3>
    <ul>
    <li><p><strong>Real-time Sonification</strong>: SonicWalk employs sonification techniques to provide auditory feedback during rehabilitative exercises, enhancing patient engagement, motivation, and ultimately, improving results.</p>
    </li>
    <li><p><strong>Cross-Platform Compatibility</strong>: Built with Python and PyQT5, SonicWalk ensures compatibility across different operating systems (<em>Windows</em>, <em>Linux</em>), maximizing accessibility and usability.</p>
    </li>
    <li><p><strong>Patient Management</strong>: Medical personnel can easily manage patients&#39; information and exercise records through SonicWalk&#39;s intuitive interface, streamlining administrative tasks and optimizing patient care.</p>
    </li>
    <li><p><strong>Exercise Monitoring</strong>: The application facilitates real-time monitoring of exercise sessions by healthcare providers, empowering proactive intervention and personalized guidance for patients throughout their rehabilitation journey.</p>
    </li>
    </ul>
    <!-- - **Data Visualization**: SonicWalk visualizes exercise data, enabling medical staff to analyze trends, identify areas for improvement, and personalize treatment plans. -->
    <h3 id="about-the-project">About the Project</h3>
    <p>SonicWalk originated as part of a final internship project for a bachelor&#39;s degree in computer science. The project involved the implementation of real-time signal analysis and processing methods to detect points of interest during exercises, as well as the complete development of the graphical user interface and data management logic.</p>
    <h3 id="contributions">Contributions</h3>
    <p>Roberto Tallarini&#39;s Contribution:</p>
    <ul>
    <li>Expanded the original Gabriele&#39;s interface to include additional rehabilitative exercises, such as high-knee marching, backward step marching, swing, and double step.</li>
    <li>Implemented real-time signals analysis methods for each exercise, including automatic leg detection and audio feedback using the Pygame library (which provides stability and is less susceptible to interference from threads or drivers compared to Simpleaudio).</li>
    <li>Implemented a bpm estimator to calculate the medium bpm of the patient.</li>
    <li>Ideated, Designed and developed a comprehensive graphical user interface for managing patients informations, records, exercise sessions, and music libraries.</li>
    <li>Conducted the porting of the original code to Windows and integrated it with the graphical user interface.</li>
    <li>Envisioned, designed, and implemented the entire user interface and data management system.</li>
    <li><a href="https://github.com/Rob00t-unimi/SonicWalk">https://github.com/Rob00t-unimi/SonicWalk</a></li>
    </ul>
    <p>Gabriele Esposito&#39;s Contribution:</p>
    <ul>
    <li>Developed a Python interface based on the Xsens Device API to communicate with MTw Awinda motion trackers.</li>
    <li>Implemented data recording and real-time plotting of motion tracking data.</li>
    <li>Created a gyroscope-based step detector capable of detecting steps during various walking speeds.</li>
    <li>Integrated sound reproduction for signaling step occurrences using simpleaudio library.</li>
    <li>Assisted in the porting process to ensure cross-platform compatibility.</li>
    <li><a href="https://github.com/xyzxyzxyzxy/SonicWalk">https://github.com/xyzxyzxyzxy/SonicWalk</a></li>
    </ul>
    <h3 id="a-simple-python-interface-based-on-the-xsens-device-api-to-communicate-with-mtw-awinda-motion-trackers-">A simple python interface based on the Xsens Device API to communicate with MTw Awinda motion trackers.</h3>
    <p>Data from MTw motion trackers can be recorded and returned as a Numpy.array (pitch angle).
    Additionally data can be plotted in real-time in a separate process as it is received from the devices.
    A step detector can be spawn to detect steps while walking. Steps are detected and counted separately for each sensor (leg).</p>
    <p>The interface is created specifically to work with two MTw Awinda sensors that need to be both detected before starting the recording. 
    Sensors produce motion tracking data in the form of Euler angles, for the specific application of step detection pitch angle only is recorded and processed.</p>
    <p>The gyroscope based step detector is capable to detect steps during very slow walk, and can work with a great range of speeds.
    To signal the occurence of a step a sound can be reproduced from a sample library. The sample library can be specified as a path to a directory containing .WAV or .mp3 samples, such samples will be reproduced sequentially in lexicographic order. 
    This gives the possibility to partition a music track into samples that will be reproduced back to back while the subject wearing the sensors is walking, at the speed the subject is walking at.</p>
    <p>A usage example can be found in the examples directory</p>
    <pre><code class="lang-python">
        duration = <span class="number">90</span>
        samplesPath = <span class="string">"../sonicwalk/audio<span class="hljs-emphasis">_samples/cammino_</span>1<span class="hljs-emphasis">_fase_</span>2"</span>
    
        <span class="operator">with</span> <span class="object">mtw.MtwAwinda</span>(<span class="number">120</span>, <span class="number">19</span>, samplesPath) <span class="operator">as</span> <span class="object">mtw</span>:
        <span class="hljs-code">        data = <span class="object">mtw</span>.mtwRecord(duration, plot=<span class="value">True</span>, analyze=<span class="value">True</span>, exType = <span class="number">0</span>, setStart=<span class="value">None</span>, calculateBpm=<span class="value">False</span>, shared_data=<span class="value">None</span>)</span>
    
        data0 = data[<span class="number">0</span>][<span class="number">0</span>]
        data1 = data[<span class="number">0</span>][<span class="number">1</span>]
        index0 = data[<span class="number">1</span>][<span class="number">0</span>]
        index1 = data[<span class="number">1</span>][<span class="number">1</span>]
    
        interestingPoints0 = data[<span class="number">2</span>][<span class="number">0</span>]
        interestingPoints1 = data[<span class="number">2</span>][<span class="number">1</span>]
    
        bpmValue = data[<span class="number">3</span>]  <span class="comment"># if calculateBpm==False will be None</span>
    </code></pre>
    
    <ul>
    <li><p><strong>exType</strong> indicates the exercise to run:</p>
    <p>  
        <ol start="0">
            <li> Walking </li>
            <li>  Walking in place (High Knees marching, backward step marching) </li>
            <li> Walking in place (High Knees with sensors on the thighs) </li>
            <li> Swing </li>
            <li> Double step </li>
        </ol>
    </p>
    </li>
    <li><p><strong>setStart</strong> is a callback function to call when the exercise starts</p>
    </li>
    <li><strong>CalculateBpm</strong> replace the real time feedback width a the calculation of medium bpm</li>
    <li><strong>shared_data</strong> is an optional pre-allocated SharedData object</li>
    </ul>
    <p>A <strong>MtwAwinda</strong> singleton object instance must be created in a <strong>with</strong> statement,
    this ensures proper setup of the sensors and closing. 
    When creating the object instance a <em>sample rate</em> and a <em>radio channel</em> must be specified together with a <em>path to the library of samples</em>. 
    For the available sample rates and radio channels consult the Xsense Device API documentation or the MTw Awinda motion trackers documentation.
    After the creation of the object the sensors and master devices are put in <em>Measurement mode</em> and recording can be started.
    To start recording the public method <strong>mtwRecord</strong> can be called, specifying a duration value that must be a positive integer indicating the number of seconds the recording should last.
    Two additional flags can be provided:</p>
    <ul>
    <li>plot: spawns a daemon that handles real time plotting (using matplotlib).</li>
    <li>analyze: spawns two daemons (one for each sensor) handling step detection and samples reproduction from the library of samples provided.</li>
    </ul>
    <p>The Recorded data returned by the <code>mtwRecord</code> function includes several components:</p>
    <ul>
    <li><code>data[0][0]</code> and <code>data[0][1]</code> are tuples of Numpy.arrays containing the pitch angle buffers for the two signals, respectively. The two buffers of length 72000 samples can contain roughly 10 minutes of recording (at 120Hz) after witch the buffers are overwritten.</li>
    <li><code>data[1][0]</code> and <code>data[1][1]</code> represent the indices at which the recording stopped for the two signals.</li>
    <li>The &#39;interesting points&#39; related to the two signals are stored in <code>data[2][0]</code> and <code>data[2][1]</code>, which are tuples of Numpy.arrays. These arrays contain the approximate indices of the two signals at which points of interest were detected.</li>
    <li><code>data[3]</code> contains the average beats per minute (bpm) value if the calculation was requested and the relevant data was acquired. Otherwise, it will be <code>False</code>.</li>
    </ul>
    <h3 id="requires-and-dependencies-installation">Requires and dependencies installation</h3>
    <p>Python version 3.9 is required, (it is recommended to create a conda virtual environment using the python version 3.9)\
    Installing the <em>xsensdeviceapi</em> dependency:</p>
    <p>Windows:</p>
    <pre><code>pip install wheels/xsensdeviceapi<span class="hljs-number">-2022.2</span><span class="hljs-number">.0</span>-cp39-none-win_amd64.whl
    </code></pre><p>Linux:</p>
    <pre><code>pip install wheels/xsensdeviceapi<span class="hljs-number">-2022.0</span><span class="hljs-number">.0</span>-cp39-none-linux_x86_64.whl
    </code></pre><h4 id="installation-of-interface-dependencies-only">Installation of Interface Dependencies Only</h4>
    <ul>
    <li>matplotlib</li>
    <li>pygame</li>
    <li><p>scipy</p>
    <p>  Installing the sonicwalk package:</p>
    <pre><code>  pip install dist/sonicwalk-<span class="hljs-number">1.0</span>.<span class="hljs-number">0</span><span class="hljs-selector-class">.dev1-py3-none-any</span><span class="hljs-selector-class">.whl</span>
    </code></pre><pre><code>  pip <span class="hljs-keyword">install</span> matplotlib pygame scipy
    </code></pre><pre><code class="lang-python">  <span class="operator">from</span> sonicwalk <span class="operator">import</span> <span class="object">mtw</span>
    </code></pre>
    <h4 id="installation-of-complete-gui-application-dependencies">Installation of Complete GUI application Dependencies</h4>
    </li>
    <li>matplotlib</li>
    <li>pygame</li>
    <li>scipy</li>
    <li>PyQt5</li>
    <li><p>qt_material</p>
    <pre><code>  pip <span class="hljs-keyword">install</span> PyQt5 matplotlib pygame qt_material scipy
    </code></pre></li>
    </ul>
    <h2 id="the-gui-sonicwalk-application">The GUI SonicWalk Application</h2>
    <h3 id="run-by-code">Run by code</h3>
    <p>Install the Complete GUI application Dependencies, then run:</p>
    <pre><code> 
    <span class="hljs-keyword">python</span> SonicWalk.<span class="hljs-keyword">py</span>
    </code></pre><h3 id="run-by-exe-windows-">Run by Exe (Windows)</h3>


</div>

<h2 id="guide">Guide to using the app</h2>
<!-- Contenuto della guida all'utilizzo dell'app -->

<h2 id="analysis">Analysis Documentation</h2>
<div id="pdfContainer"></div>

<!-- <h2 id="credits">Credits</h2> -->
<!-- Contenuto dei credits dell'applicazione -->

<script>
    var pdf_path = "../report/Step_detection_and_sonification_from_gyroscope_sensor_data.pdf";

    var iframe = document.createElement('iframe');
    iframe.style.width = '100%';
    iframe.style.height = '100%';
    iframe.src = pdf_path;

    document.getElementById('pdfContainer').appendChild(iframe);
</script>

</body>
</html>
